{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I52tRuW6ec6w"
   },
   "source": [
    "## Preprocessing Pipeline Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DqFpjlFJgANq"
   },
   "source": [
    "# Initial Setup via Dr. Tang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2198,
     "status": "ok",
     "timestamp": 1737955109931,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "22GphpDAdViB",
    "outputId": "ba999cb8-860e-4429-fda3-a2ef4654fd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# If you would like to save and read data files from your Google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 6752,
     "status": "ok",
     "timestamp": 1737955116682,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "Pge3clrOfLfl"
   },
   "outputs": [],
   "source": [
    "# To load the example data set\n",
    "tw_df = pd.read_csv('/content/drive/MyDrive/2024 Spring/Text Mining/Week 2/LucasHoskinAssignment/tweeter_training.csv', encoding='ISO-8859-1', header=None)\n",
    "column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']\n",
    "tw_df.columns = column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 153,
     "status": "ok",
     "timestamp": 1737955117111,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "i3bV9KO8fdEJ",
    "outputId": "53c2ae1f-3b70-4473-9045-b6c54d504ac4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@switchfoot http://twitpic.com/2y1zl - Awww, that'\n"
     ]
    }
   ],
   "source": [
    "# Combine all text into 'raw text' and print as a test\n",
    "raw_text = ' '.join(tw_df['text'])\n",
    "print(raw_text[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyDCoaMDgIn7"
   },
   "source": [
    "# Creating and Testing the Tokenizer (Extracting Numbers and Symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21dE_r0mBMEJ"
   },
   "source": [
    "### Creating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1737955473631,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "mKWLdXazg-Pi"
   },
   "outputs": [],
   "source": [
    "def ExtractNumsAndSymbols(all_text):\n",
    "    \"\"\"\n",
    "    Extracts sets of numbers and singular symbols from all_text\n",
    "\n",
    "    Parameters:\n",
    "        all_text (str): One string containing all text\n",
    "\n",
    "    Returns:\n",
    "        set_nums (list of str): list of extracted sets of numbers\n",
    "        set_symbols (list of str): list of extracted singular symbols\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists for numbers and symbols\n",
    "    set_nums = []\n",
    "    set_syms = []\n",
    "\n",
    "    # Flag that tracks if the last character (char) was a number\n",
    "    was_num = False\n",
    "\n",
    "    # Iterate through each character\n",
    "    for char in all_text:\n",
    "\n",
    "        # If number and last character was number, append to last number\n",
    "        # If number and last character was not number, start new number\n",
    "        if char.isdigit():\n",
    "            if was_num == True:\n",
    "                set_nums[-1] += char\n",
    "            else:\n",
    "                set_nums.append(char)\n",
    "            was_num = True\n",
    "\n",
    "        # If symbol, add to new string and reset flag\n",
    "        elif not char.isalnum() and not char.isspace():\n",
    "            set_syms.append(char)\n",
    "            was_num = False\n",
    "        # If not symbol or number, reset flag\n",
    "        else:\n",
    "            was_num = False\n",
    "\n",
    "    return set_nums, set_syms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzmxBLlqyrMl"
   },
   "source": [
    "### Testing a small set of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1737955477234,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "4pvkt-9WoQ45",
    "outputId": "6209ccc1-b5e3-4f16-94bc-50301ae315cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['89482', '48', '4', '8', '54661', '9', '84', '8']\n",
      "['/', '/', '/', '*', '&']\n"
     ]
    }
   ],
   "source": [
    "# Test a random piece of text\n",
    "text_test = '/hello//  siudfh lau89482u uansk48ngfadn4p8ajp*JAPRIONF54661PISUFHPsih 9f84PW&thp8'\n",
    "numbers_test, symbols_test = ExtractNumsAndSymbols(text_test)\n",
    "print(numbers_test)\n",
    "print(symbols_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URDWyfUuyyvQ"
   },
   "source": [
    "### Testing the tweeter_training.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10575,
     "status": "ok",
     "timestamp": 1737955490781,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "o_yXzJj9vWvu",
    "outputId": "0f524902-8c08-4e33-da3c-6003c1ea143a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "top 20 Numbers and Frequencies:\n",
      "2 : 69177\n",
      "3 : 53939\n",
      "4 : 36776\n",
      "1 : 36362\n",
      "5 : 22683\n",
      "6 : 22541\n",
      "7 : 18742\n",
      "8 : 15548\n",
      "0 : 12534\n",
      "10 : 10740\n",
      "9 : 10665\n",
      "30 : 8991\n",
      "12 : 6158\n",
      "11 : 5679\n",
      "20 : 5267\n",
      "100 : 4897\n",
      "13 : 4741\n",
      "15 : 4570\n",
      "09 : 3435\n",
      "24 : 3160\n",
      "\n",
      "top 20 Symbols and Frequencies:\n",
      ". : 2087151\n",
      "! : 917950\n",
      "@ : 798682\n",
      "' : 646745\n",
      ", : 486760\n",
      "/ : 261643\n",
      "? : 247562\n",
      "; : 166822\n",
      "- : 156793\n",
      "& : 146413\n",
      ": : 139057\n",
      "_ : 90166\n",
      ") : 48458\n",
      "# : 45380\n",
      "( : 42818\n",
      "* : 38339\n",
      "= : 11445\n",
      "~ : 9052\n",
      "$ : 7707\n",
      "Â¿ : 7639\n"
     ]
    }
   ],
   "source": [
    "# Running the Tokenizer\n",
    "numbers_tweet_text, symbols_tweet_text = ExtractNumsAndSymbols(raw_text)\n",
    "\n",
    "# Show top 20 values of numbers and symbols from the tweeter_training.csv file\n",
    "top_nums = Counter(numbers_tweet_text).most_common(20)\n",
    "print(\"\\ntop 20 Numbers and Frequencies:\")\n",
    "for number, count in top_nums:\n",
    "    print(number, \":\", count)\n",
    "\n",
    "top_syms = Counter(symbols_tweet_text).most_common(20)\n",
    "print(\"\\ntop 20 Symbols and Frequencies:\")\n",
    "for symbol, count in top_syms:\n",
    "    print(symbol, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VAmKJz9j1OqW"
   },
   "source": [
    "# Creating and Testing Another Tokenizer (Extracting All Three Character Subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7BXPPS7BQPd"
   },
   "source": [
    "### Creating the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 89,
     "status": "ok",
     "timestamp": 1737955120646,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "uyvsCXSD1O5s"
   },
   "outputs": [],
   "source": [
    "def ExtractSetsOfThree(all_text):\n",
    "    \"\"\"\n",
    "    Extracts strings that include each character and its following two characters\n",
    "\n",
    "    Example:\n",
    "        input: testing123\n",
    "        output: ['tes', 'est', 'sti', 'tin', 'ing', 'ng1', 'g12', '123']\n",
    "\n",
    "    Parameters:\n",
    "        all_text (str): One string containing all text\n",
    "\n",
    "    Returns:\n",
    "        sets (list of str): extracted sets of three characters\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize sets of strings\n",
    "    sets = []\n",
    "\n",
    "\n",
    "    # Iterate through each character (except the last 2)\n",
    "    for i in range(len(all_text) - 2):\n",
    "        substring = all_text[i:i+3]\n",
    "        sets.append(substring)\n",
    "\n",
    "    return sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-W3grGOAANP"
   },
   "source": [
    "### Testing a small set of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 99,
     "status": "ok",
     "timestamp": 1737955121783,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "iqm_GrOx3Mtm",
    "outputId": "9c1e206d-d605-4abe-ede8-1199cb1a85fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/he', 'hel', 'ell', 'llo', 'lo/', 'o//', '// ', '/  ']\n"
     ]
    }
   ],
   "source": [
    "# Test a random piece of text\n",
    "text_test = '/hello//  siudfh lau89482u uansk48ngfadn4p8ajp*JAPRIONF54661PISUFHPsih 9f84PW&thp8'\n",
    "subsets_test = ExtractSetsOfThree(text_test)\n",
    "print(subsets_test[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NszMWFacAgvg"
   },
   "source": [
    "### Testing the tweeter_training.csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54299,
     "status": "ok",
     "timestamp": 1737955177198,
     "user": {
      "displayName": "lhosk",
      "userId": "12243202300103871421"
     },
     "user_tz": 300
    },
    "id": "PU4azy7g3Y48",
    "outputId": "0ca247a7-f9bb-413f-8e3c-7feb7cff53ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Sets and Frequencies:\n",
      " th : 1083248\n",
      "ing : 849115\n",
      " to : 796095\n",
      "ng  : 756348\n",
      "the : 745486\n",
      "to  : 578320\n",
      "he  : 576971\n",
      "  @ : 522450\n",
      " I  : 496608\n",
      "nd  : 418627\n",
      " an : 383456\n",
      "... : 379842\n",
      " ha : 376316\n",
      "er  : 369208\n",
      " a  : 366212\n",
      "ed  : 357901\n",
      ".   : 355115\n",
      "and : 354033\n",
      "you : 348543\n",
      " yo : 343839\n"
     ]
    }
   ],
   "source": [
    "# Running the Tokenizer\n",
    "subsets_tweet_text = ExtractSetsOfThree(raw_text)\n",
    "\n",
    "# Show top 20 subsets from the tweeter_training.csv file\n",
    "top_subsets = Counter(subsets_tweet_text).most_common(20)\n",
    "print(\"\\nTop Sets and Frequencies:\")\n",
    "for set, count in top_subsets:\n",
    "    print(set, \":\", count)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMZdf3Enn25EX7OgsiwSAFC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
